# transformer-tf
Attention Is All You Need implemented as Tensorflow 2.0

## Requirements
Tensorflow == 2.0_alpha <br>
Python == 3.6

## Data
WMT'14 English-German data: https://nlp.stanford.edu/projects/nmt/

Download the datasets using the following script:
```
./download.sh
```

## Results
|         | Train Set    | Validation Set    | Test Set |
|---------|--------------|-------------------|----------|
| Model   | --%          | --%               | --%      |

## Reference
[Attention Is All You Need](https://arxiv.org/abs/1706.03762)<br>
[The Annotated Transformer implemented as Pytorch](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
