# transformer-tf
- [X] Attention Is All You Need implemented as Tensorflow 2.0<br>
- [ ] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implemented as Tensorflow 2.0<br>
- [ ] Universal Transformers implemented as Tensorflow 2.0

## Requirements
Tensorflow == 2.0_alpha <br>
Python == 3.6

## Data
WMT'14 English-German data: https://nlp.stanford.edu/projects/nmt/

Download the datasets using the following script:
```
./download.sh
```

## Results
|         | Train Set    | Validation Set    | Test Set |
|---------|--------------|-------------------|----------|
| Model   | --%          | --%               | --%      |

## Reference
[Attention Is All You Need](https://arxiv.org/abs/1706.03762)<br>
[Universal Transformers](https://arxiv.org/abs/1807.03819)
[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
